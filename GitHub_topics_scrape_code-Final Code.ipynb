{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eb50e0c",
   "metadata": {},
   "source": [
    "## Scraping Top Repos for Topics on GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccc502c",
   "metadata": {},
   "source": [
    "### Here are the steps we'll follow:\n",
    "\n",
    "- We're going to scrape https://github.com/topics\n",
    "- We'll get a list of topics. For each topic, we'll get topic title, topic page URL and topic description\n",
    "- For each topic, we'll get the top 25 repositories in the topic from the topic page\n",
    "- For each repository, we'll grab the repo name, username, stars and repo URL\n",
    "- For each topic we'll create a CSV file in the following format:\n",
    "\n",
    "```\n",
    "Repo Name,Username,Stars,Repo URL\n",
    "three.js,mrdoob,69700,https://github.com/mrdoob/three.js\n",
    "libgdx,libgdx,18300,https://github.com/libgdx/libgdx\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef113f44",
   "metadata": {},
   "source": [
    "## Scrape the list of topics from Github\n",
    "\n",
    "Explain how you'll do it.\n",
    "\n",
    "- use requests to downlaod the page\n",
    "- user BS4 to parse and extract information\n",
    "- convert to a Pandas dataframe\n",
    "\n",
    "Let's write a function to download the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "547f1823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_topics_page(topic_url):\n",
    "    topics_url ='https://github.com/topics'\n",
    "    response = requests.get(topics_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('failed to load page {}'.format(topic_url))\n",
    "        doc = BeautifulSoup(response.textxt,'html.parser')\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4852de",
   "metadata": {},
   "source": [
    "#### def get_topic_titles(doc) gives list of titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d3d084d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_titles(doc):\n",
    "    selection_class = 'f3 lh-condensed mb-0 mt-1 Link--primary'\n",
    "    topic_title_tags = doc.find_all('p', {'class':selection_class }) \n",
    "    topic_titles =[]\n",
    "    for tag in topic_title_tags:\n",
    "        topic_titles.append(tag.text)\n",
    "    return topic_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e25f0f",
   "metadata": {},
   "source": [
    "#### Similarly we have defined functions for descriptions and URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e38921cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_description(doc):\n",
    "    desc_selector = 'f5 color-fg-muted mb-0 mt-1'\n",
    "    topic_desc_tags = doc.find_all('p', {'class':desc_selector })\n",
    "    topic_descs = []\n",
    "    for tag in topic_desc_tags:\n",
    "        topic_descs.append(tag.text.strip())\n",
    "    return topic_descs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4e02dacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_url(doc):\n",
    "    topic_link_tags = doc.find_all('a',{'class':'no-underline flex-1 d-flex flex-column'})\n",
    "    topic_urls = []\n",
    "    base_url = 'https://github.com'\n",
    "    for tag in topic_link_tags:\n",
    "        topic_urls.append(base_url + tag['href'])\n",
    "    return topic_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b179854",
   "metadata": {},
   "source": [
    "### Combining all function together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bf415ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics():\n",
    "    topics_url ='https://github.com/topics'\n",
    "    response = requests.get(topics_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('failed to load page {}'.format(topic_url))\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    topics_dict = {\n",
    "        'title'      : get_topic_titles(doc),\n",
    "        'description' : get_topic_description(doc),\n",
    "        'url'         : get_topic_url(doc)   \n",
    "    }\n",
    "    return pd.DataFrame(topics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050f0f0a",
   "metadata": {},
   "source": [
    "## Get the top repositories from a topic page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b13598b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_page(topic_url):\n",
    "    #Download the page\n",
    "    response = requests.get(topic_url)\n",
    "    #check successful response\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('failed to load page {}'.format(topic_url))\n",
    "    # Parse using Beautifulsoup\n",
    "    topic_doc = BeautifulSoup(response.text,'html.parser')\n",
    "    return topic_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "47dfd22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = get_topic_page('https://github.com/topics/3d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa807cd",
   "metadata": {},
   "source": [
    "#### extracting tags for title, urls, stars..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4d10eca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def star_parser(star_num):\n",
    "    if star_num[-1] =='k':\n",
    "        return float(star_num[:-1]) * 1000\n",
    "    return int(star_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "51c9e538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_info(h1_tag,star_tag):\n",
    "   #returns all the required information about a repository\n",
    "    a_tags    = h1_tag.find_all('a')\n",
    "    username  = a_tags[0].text.strip()\n",
    "    repo_name = a_tags[1].text.strip()\n",
    "    repo_url  = base_url + a_tags[1]['href']\n",
    "    stars     = star_parser(star_tag.text.strip())\n",
    "    return username, repo_name, repo_url, stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ac06a3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_repo(topic_doc):\n",
    "    # Get h1 tags containing repo > title, username, url\n",
    "    h1_selection_class = 'f3 color-fg-muted text-normal lh-condensed'\n",
    "    repo_tag = topic_doc.find_all('h3',{'class':h1_selection_class})\n",
    "    # Get Star Tag\n",
    "    star_tag = topic_doc.find_all('span',{'class':\"Counter js-social-count\"})\n",
    "    # Get repo info\n",
    "    \n",
    "    topic_repos_dict = {'username' : [],'repo_name': [],'stars': [],'repo_url' : []    }\n",
    "    for i in range(len((repo_tag))):\n",
    "        repo_info = get_repo_info(repo_tag[i],star_tag[i])\n",
    "        topic_repos_dict['username'].append(repo_info[0])\n",
    "        topic_repos_dict['repo_name'].append(repo_info[1])\n",
    "        topic_repos_dict['repo_url'].append(repo_info[2])   \n",
    "        topic_repos_dict['stars'].append(repo_info[3])\n",
    "\n",
    "    return pd.DataFrame(topic_repos_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960ca6bb",
   "metadata": {},
   "source": [
    "## creating CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2ff3c709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topic(topic_url,topic_name):\n",
    "    fname = 'D:/' + topic_name + '.csv'\n",
    "    if os.path.exists(fname):\n",
    "        print('this file already exists skipping.......'.format(fname))\n",
    "        return\n",
    "    topic_df = get_topic_repo(get_topic_page(topic_url))\n",
    "    topic_df.to_csv(fname, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d377fdc1",
   "metadata": {},
   "source": [
    "## Finally combining all together :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3f4f1b",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "- We have a funciton to get the list of topics\n",
    "- We have a function to create a CSV file for scraped repos from a topics page\n",
    "- Let's create a function to put them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "16a18353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics_repos():\n",
    "    print('scraping list of topics')\n",
    "    topics_df = scrape_topics()\n",
    "    for index, row in topics_df.iterrows():\n",
    "        print('scraping top repositories for \"{}\"'.format(row['title']))\n",
    "        scrape_topic(row['url'],row['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33c042f",
   "metadata": {},
   "source": [
    "Run above function to scrape the top repos for the all the topics on the first page of https://github.com/topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "11fe8e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping list of topics\n",
      "scraping top repositories for \"3D\"\n",
      "this file already exists skipping.......\n",
      "scraping top repositories for \"Ajax\"\n",
      "this file already exists skipping.......\n",
      "scraping top repositories for \"Algorithm\"\n",
      "this file already exists skipping.......\n",
      "scraping top repositories for \"Amp\"\n",
      "this file already exists skipping.......\n",
      "scraping top repositories for \"Android\"\n",
      "this file already exists skipping.......\n",
      "scraping top repositories for \"Angular\"\n",
      "this file already exists skipping.......\n",
      "scraping top repositories for \"Ansible\"\n",
      "this file already exists skipping.......\n",
      "scraping top repositories for \"API\"\n",
      "this file already exists skipping.......\n",
      "scraping top repositories for \"Arduino\"\n",
      "this file already exists skipping.......\n",
      "scraping top repositories for \"ASP.NET\"\n",
      "this file already exists skipping.......\n",
      "scraping top repositories for \"Atom\"\n",
      "this file already exists skipping.......\n",
      "scraping top repositories for \"Awesome Lists\"\n",
      "this file already exists skipping.......\n",
      "scraping top repositories for \"Amazon Web Services\"\n",
      "this file already exists skipping.......\n",
      "scraping top repositories for \"Azure\"\n",
      "this file already exists skipping.......\n",
      "scraping top repositories for \"Babel\"\n",
      "this file already exists skipping.......\n",
      "scraping top repositories for \"Bash\"\n",
      "this file already exists skipping.......\n",
      "scraping top repositories for \"Bitcoin\"\n",
      "this file already exists skipping.......\n",
      "scraping top repositories for \"Bootstrap\"\n",
      "this file already exists skipping.......\n",
      "scraping top repositories for \"Bot\"\n",
      "this file already exists skipping.......\n",
      "scraping top repositories for \"C\"\n",
      "this file already exists skipping.......\n",
      "scraping top repositories for \"Chrome\"\n",
      "this file already exists skipping.......\n",
      "scraping top repositories for \"Chrome extension\"\n",
      "this file already exists skipping.......\n",
      "scraping top repositories for \"Command line interface\"\n",
      "this file already exists skipping.......\n",
      "scraping top repositories for \"Clojure\"\n",
      "this file already exists skipping.......\n",
      "scraping top repositories for \"Code quality\"\n",
      "this file already exists skipping.......\n",
      "scraping top repositories for \"Code review\"\n",
      "this file already exists skipping.......\n",
      "scraping top repositories for \"Compiler\"\n",
      "this file already exists skipping.......\n",
      "scraping top repositories for \"Continuous integration\"\n",
      "this file already exists skipping.......\n",
      "scraping top repositories for \"COVID-19\"\n",
      "this file already exists skipping.......\n",
      "scraping top repositories for \"C++\"\n",
      "this file already exists skipping.......\n"
     ]
    }
   ],
   "source": [
    "scrape_topics_repos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98c0f55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e11893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c0be6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc1b8d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df469ced",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
